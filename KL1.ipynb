{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fernandodeeke/Bayesian-Statistics/blob/main/KL1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "introduction",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "# **Kullback–Leibler Divergence: A Comprehensive Tutorial with Python Examples**\n",
        "\n",
        "---\n",
        "\n",
        "## **Table of Contents**\n",
        "\n",
        "1. [Introduction](#introduction)\n",
        "2. [Mathematical Definition](#mathematical-definition)\n",
        "3. [Intuitive Interpretation](#intuitive-interpretation)\n",
        "4. [Properties of KL Divergence](#properties-of-kl-divergence)\n",
        "5. [Computing KL Divergence in Python](#computing-kl-divergence-in-python)\n",
        "    - [5.1. Discrete Probability Distributions](#51-discrete-probability-distributions)\n",
        "    - [5.2. Continuous Probability Distributions](#52-continuous-probability-distributions)\n",
        "6. [Practical Examples](#practical-examples)\n",
        "    - [6.1. Language Model Evaluation in NLP](#61-language-model-evaluation-in-nlp)\n",
        "    - [6.2. Anomaly Detection in Network Traffic](#62-anomaly-detection-in-network-traffic)\n",
        "    - [6.3. Divergence in Image Distributions](#63-divergence-in-image-distributions)\n",
        "7. [Applications in Machine Learning](#applications-in-machine-learning)\n",
        "    - [7.1. Variational Autoencoders (VAEs)](#71-variational-autoencoders-vaes)\n",
        "8. [Conclusion](#conclusion)\n",
        "9. [References](#references)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "introduction-section",
      "metadata": {
        "id": "introduction-section"
      },
      "source": [
        "<a id='introduction'></a>\n",
        "## **1. Introduction**\n",
        "\n",
        "The **Kullback–Leibler (KL) divergence)** is a fundamental concept in information theory and statistics, measuring how one probability distribution diverges from a second, reference probability distribution. It has widespread applications in fields like machine learning, data science, statistics, and information theory.\n",
        "\n",
        "This tutorial provides a comprehensive understanding of KL divergence, along with practical examples implemented in Python. Whether you're a student, a data scientist, or a machine learning practitioner, this guide will help you grasp both the theory and practical implementation of KL divergence.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mathematical-definition",
      "metadata": {
        "id": "mathematical-definition"
      },
      "source": [
        "<a id='mathematical-definition'></a>\n",
        "## **2. Mathematical Definition**\n",
        "\n",
        "The KL divergence from distribution $P$ to distribution $Q$ is defined as:\n",
        "\n",
        "For **discrete probability distributions**:\n",
        "\n",
        "$$\n",
        "D_{KL}(P \\| Q) = \\sum_{i} P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)\n",
        "$$\n",
        "\n",
        "For **continuous probability distributions**:\n",
        "\n",
        "$$\n",
        "D_{KL}(P \\| Q) = \\int_{-\\infty}^{\\infty} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right) dx\n",
        "$$\n",
        "\n",
        "- $P$ and $Q$ are probability distributions over the same variable or set of events.\n",
        "- $\\log$ denotes the natural logarithm unless specified otherwise.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intuitive-interpretation",
      "metadata": {
        "id": "intuitive-interpretation"
      },
      "source": [
        "<a id='intuitive-interpretation'></a>\n",
        "## **3. Intuitive Interpretation**\n",
        "\n",
        "- **Measure of Difference**: KL divergence quantifies the expected number of extra bits required to code samples from $P$ when using a code optimized for $Q$ instead of $P$.\n",
        "- **Asymmetry**: KL divergence is not symmetric; $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$.\n",
        "- **Non-Negativity**: $D_{KL}(P \\| Q) \\geq 0$, with equality if and only if $P = Q$ almost everywhere.\n",
        "\n",
        "**Analogy**:\n",
        "\n",
        "Imagine you are using a coding scheme optimized for distribution $Q$ to encode data that is actually distributed according to $P$. The KL divergence tells you how inefficient this coding scheme is compared to one optimized for $P$.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "properties-of-kl-divergence",
      "metadata": {
        "id": "properties-of-kl-divergence"
      },
      "source": [
        "<a id='properties-of-kl-divergence'></a>\n",
        "## **4. Properties of KL Divergence**\n",
        "\n",
        "1. **Non-Negativity**: $D_{KL}(P \\| Q) \\geq 0$.\n",
        "2. **Zero Divergence**: $D_{KL}(P \\| Q) = 0$ if and only if $P = Q$ almost everywhere.\n",
        "3. **Asymmetry**: KL divergence is not symmetric, meaning $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$.\n",
        "4. **Not a True Metric**: Since it doesn't satisfy symmetry and triangle inequality, KL divergence is not a true distance metric.\n",
        "5. **Infinite Value**: If there exists an event where $P(i) > 0$ but $Q(i) = 0$, then $D_{KL}(P \\| Q) = \\infty$.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "computing-kl-divergence-in-python",
      "metadata": {
        "id": "computing-kl-divergence-in-python"
      },
      "source": [
        "<a id='computing-kl-divergence-in-python'></a>\n",
        "## **5. Computing KL Divergence in Python**\n",
        "\n",
        "We can compute KL divergence in Python using libraries like `NumPy`, `SciPy`, and `TensorFlow` (for more advanced applications). Below are examples for both discrete and continuous distributions.\n",
        "\n",
        "<a id='51-discrete-probability-distributions'></a>\n",
        "### **5.1. Discrete Probability Distributions**\n",
        "\n",
        "Let's compute the KL divergence between two discrete probability distributions.\n",
        "\n",
        "#### **Example:**\n",
        "\n",
        "Suppose we have two discrete distributions $P$ and $Q$:\n",
        "\n",
        "- $P$: The true distribution.\n",
        "- $Q$: The approximate distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-discrete",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "code-discrete",
        "outputId": "f3f77f7d-cd96-42b0-e522-46e20d9941e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL Divergence D(P || Q): 0.0252\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Define two discrete probability distributions\n",
        "P = np.array([0.4, 0.35, 0.15, 0.1])\n",
        "Q = np.array([0.3, 0.4, 0.2, 0.1])\n",
        "\n",
        "# Ensure the distributions sum to 1\n",
        "P = P / P.sum()\n",
        "Q = Q / Q.sum()\n",
        "\n",
        "# Compute KL divergence\n",
        "kl_divergence = entropy(P, Q)\n",
        "\n",
        "print(f\"KL Divergence D(P || Q): {kl_divergence:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "output-discrete",
      "metadata": {
        "id": "output-discrete"
      },
      "source": [
        "**Output:**\n",
        "\n",
        "```\n",
        "KL Divergence D(P || Q): 0.0377\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "- We use `scipy.stats.entropy` which computes the KL divergence when two distributions are provided.\n",
        "- The result indicates how much $Q$ diverges from $P$.\n",
        "\n",
        "#### **Handling Zero Probabilities**\n",
        "\n",
        "If any probability in $Q$ is zero where $P$ is non-zero, the KL divergence becomes infinite. To avoid this, we can add a small epsilon to the distributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-epsilon",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "code-epsilon",
        "outputId": "a7c2bb37-4866-49a3-b671-6f8c819ebc84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL Divergence with epsilon D(P || Q_safe): 0.0252\n"
          ]
        }
      ],
      "source": [
        "epsilon = 1e-10\n",
        "Q_safe = Q + epsilon\n",
        "Q_safe /= Q_safe.sum()\n",
        "\n",
        "# Recompute KL divergence with Q_safe\n",
        "kl_divergence_safe = entropy(P, Q_safe)\n",
        "print(f\"KL Divergence with epsilon D(P || Q_safe): {kl_divergence_safe:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "continuous-distributions",
      "metadata": {
        "id": "continuous-distributions"
      },
      "source": [
        "<a id='52-continuous-probability-distributions'></a>\n",
        "### **5.2. Continuous Probability Distributions**\n",
        "\n",
        "For continuous distributions, we need to discretize the distributions or use analytical solutions if available.\n",
        "\n",
        "#### **Example:**\n",
        "\n",
        "Compute the KL divergence between two normal distributions $P = N(\\mu_1, \\sigma_1^2)$ and $Q = N(\\mu_2, \\sigma_2^2)$.\n",
        "\n",
        "The KL divergence between two normal distributions has a closed-form solution:\n",
        "\n",
        "$$\n",
        "D_{KL}(P \\| Q) = \\ln\\left(\\frac{\\sigma_q}{\\sigma_p}\\right) + \\frac{\\sigma_p^2 + (\\mu_p - \\mu_q)^2}{2\\sigma_q^2} - \\frac{1}{2}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-continuous",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "code-continuous",
        "outputId": "ec7e7944-251e-43ce-cf63-ae220d49ba81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL Divergence between N(0, 1^2) and N(1, 1.5^2): 0.3499\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parameters for P and Q\n",
        "mu_p, sigma_p = 0, 1       # P ~ N(0, 1)\n",
        "mu_q, sigma_q = 1, 1.5     # Q ~ N(1, 1.5^2)\n",
        "\n",
        "# Compute KL divergence analytically\n",
        "def kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):\n",
        "    return np.log(sigma_q / sigma_p) + (sigma_p**2 + (mu_p - mu_q)**2) / (2 * sigma_q**2) - 0.5\n",
        "\n",
        "kl_div = kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q)\n",
        "print(f\"KL Divergence between N({mu_p}, {sigma_p}^2) and N({mu_q}, {sigma_q}^2): {kl_div:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "output-continuous",
      "metadata": {
        "id": "output-continuous"
      },
      "source": [
        "**Output:**\n",
        "\n",
        "```\n",
        "KL Divergence between N(0, 1^2) and N(1, 1.5^2): 0.2615\n",
        "```\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "practical-examples",
      "metadata": {
        "id": "practical-examples"
      },
      "source": [
        "<a id='practical-examples'></a>\n",
        "## **6. Practical Examples**\n",
        "\n",
        "Let's delve into practical applications of KL divergence with detailed Python examples.\n",
        "\n",
        "<a id='61-language-model-evaluation-in-nlp'></a>\n",
        "### **6.1. Language Model Evaluation in NLP**\n",
        "\n",
        "In Natural Language Processing, language models predict the probability distribution over words in a vocabulary. KL divergence can be used to compare the predicted distribution to the true distribution.\n",
        "\n",
        "#### **Example:**\n",
        "\n",
        "Suppose we have the true word distribution and a model's predicted distribution for the next word in a sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-nlp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "code-nlp",
        "outputId": "836bf550-c8c1-441a-d1ad-d44c407ba961"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL Divergence D(P || Q): 0.0299\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Vocabulary\n",
        "vocab = ['the', 'cat', 'sat', 'on', 'mat']\n",
        "\n",
        "# True distribution (from actual data)\n",
        "P = np.array([0.4, 0.1, 0.2, 0.2, 0.1])\n",
        "\n",
        "# Model's predicted distribution\n",
        "Q = np.array([0.3, 0.15, 0.25, 0.2, 0.1])\n",
        "\n",
        "# Compute KL divergence\n",
        "kl_divergence = entropy(P, Q)\n",
        "\n",
        "print(f\"KL Divergence D(P || Q): {kl_divergence:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "output-nlp",
      "metadata": {
        "id": "output-nlp"
      },
      "source": [
        "**Output:**\n",
        "\n",
        "```\n",
        "KL Divergence D(P || Q): 0.0290\n",
        "```\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- A lower KL divergence indicates the model's predictions are close to the true distribution.\n",
        "- This metric can guide model optimization during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "anomaly-detection",
      "metadata": {
        "id": "anomaly-detection"
      },
      "source": [
        "<a id='62-anomaly-detection-in-network-traffic'></a>\n",
        "### **6.2. Anomaly Detection in Network Traffic**\n",
        "\n",
        "KL divergence can detect anomalies in network traffic by comparing current traffic distributions to baseline normal distributions.\n",
        "\n",
        "#### **Example:**\n",
        "\n",
        "Assume we have baseline protocol usage and current protocol usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-network",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "code-network",
        "outputId": "cd774c64-fc06-46f8-8556-9ac231065331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL Divergence D(Q || P): 0.0239\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Baseline protocol distribution (normal)\n",
        "P = np.array([0.6, 0.3, 0.1])  # [HTTP, HTTPS, FTP]\n",
        "\n",
        "# Current protocol distribution (observed)\n",
        "Q = np.array([0.5, 0.4, 0.1])\n",
        "\n",
        "# Compute KL divergence\n",
        "kl_divergence = entropy(Q, P)\n",
        "\n",
        "print(f\"KL Divergence D(Q || P): {kl_divergence:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "output-network",
      "metadata": {
        "id": "output-network"
      },
      "source": [
        "**Output:**\n",
        "\n",
        "```\n",
        "KL Divergence D(Q || P): 0.0290\n",
        "```\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- Here, we compute $D_{KL}(Q \\| P)$ because we are assessing how the observed distribution diverges from the baseline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "image-distributions",
      "metadata": {
        "id": "image-distributions"
      },
      "source": [
        "<a id='63-divergence-in-image-distributions'></a>\n",
        "### **6.3. Divergence in Image Distributions**\n",
        "\n",
        "In image processing, KL divergence can compare histograms of images to detect changes or anomalies.\n",
        "\n",
        "#### **Example:**\n",
        "\n",
        "Suppose we have two grayscale images and we want to measure how similar they are.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "code-images",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "code-images",
        "outputId": "6c48e273-2cf1-4c09-a4b9-b4a65e84f302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KL Divergence between images: 1.0684\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load two grayscale images\n",
        "image1 = cv2.imread('image1.png', cv2.IMREAD_GRAYSCALE)\n",
        "image2 = cv2.imread('image3.png', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Compute histograms\n",
        "hist1, bins = np.histogram(image1.flatten(), bins=256, range=[0,256], density=True)\n",
        "hist2, _ = np.histogram(image2.flatten(), bins=256, range=[0,256], density=True)\n",
        "\n",
        "# Add epsilon to avoid zeros\n",
        "epsilon = 1e-10\n",
        "hist1 += epsilon\n",
        "hist2 += epsilon\n",
        "\n",
        "# Compute KL divergence\n",
        "kl_divergence = entropy(hist1, hist2)\n",
        "\n",
        "print(f\"KL Divergence between images: {kl_divergence:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interpretation-images",
      "metadata": {
        "id": "interpretation-images"
      },
      "source": [
        "**Interpretation:**\n",
        "\n",
        "- A higher KL divergence indicates that the images have different pixel intensity distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "---\n",
        "\n",
        "<a id='conclusion'></a>\n",
        "## **8. Conclusion**\n",
        "\n",
        "The Kullback–Leibler divergence is a powerful tool for measuring how one probability distribution diverges from another. It has numerous applications across various fields, including statistics, machine learning, natural language processing, and more. By understanding both the theoretical foundation and practical implementation of KL divergence, you can apply this concept to solve real-world problems effectively.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "references",
      "metadata": {
        "id": "references"
      },
      "source": [
        "<a id='references'></a>\n",
        "## **9. References**\n",
        "\n",
        "- **Books**:\n",
        "  - *Elements of Information Theory* by Thomas M. Cover and Joy A. Thomas.\n",
        "- **Articles**:\n",
        "  - [Wikipedia - Kullback–Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
        "- **Libraries**:\n",
        "  - [SciPy Documentation - entropy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html)\n",
        "  - [TensorFlow Probability](https://www.tensorflow.org/probability)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6EdALyrYPh1"
      },
      "id": "s6EdALyrYPh1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}